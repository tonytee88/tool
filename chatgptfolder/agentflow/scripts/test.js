const fs = require('fs');
const path = require('path');
const axios = require('axios');
const channelId = process.env.SLACK_CHANNEL_ID || "x"; // Capture dynamically

async function executeLLMFlow(flowData) {
    console.log("ğŸš€ Starting Flow Execution...");
  
    if (!flowData || !flowData.length) {
      console.error("âŒ No valid flowData received.");
      return;
    }
  
    const structuredFlow = flowData[0]?.flowData?.drawflow?.Home?.data || flowData[0]?.flowData?.drawflow?.data;
    if (!structuredFlow || typeof structuredFlow !== "object") {
        console.error("âŒ Invalid flowData format! Expected an object but got:", structuredFlow);
        return;
    }
    
    console.log("âœ… Valid structured flow data loaded!");

    const executionId = `exec_${Date.now()}`; // ğŸŒŸ Generate unique executionId
    console.log("ğŸ”„ Generated Execution ID:", executionId); // ğŸŒŸ Debugging executionId

    const executionOrder = determineExecutionOrder(structuredFlow);
    const storedResponses = {}; 
  
    for (const nodeId of executionOrder) {
      const currentNode = structuredFlow[nodeId];
      if (!currentNode) continue;
  
      if (currentNode.name === 'LLM Call') {
        console.log(`ğŸš€ Processing LLM Call Node: ${nodeId}`);
  
        if (storedResponses[nodeId]) { 
          console.log(`âœ… Using cached response for Node ${nodeId}`);
          currentNode.data.output = storedResponses[nodeId];
        } else {
          await waitForInputs(nodeId, structuredFlow);
  
          const combinedInputs = getSortedInputs(nodeId, structuredFlow);
          console.log("ğŸ“ Combined Inputs:", combinedInputs);
  
          const selectedModel = currentNode.data.selectedModel || 'openai/gpt-4o-mini';
          console.log(`ğŸ“ Model selected for Node ${nodeId}: ${selectedModel}`);
  
          try {
            const response = await callLLMAPI(combinedInputs, selectedModel);
            const messageResponse = response.data?.completions?.[selectedModel]?.completion?.choices?.[0]?.message?.content || "âš ï¸ No valid response";
  
            console.log(`âœ… LLM Call Node (${nodeId}) Response:`, messageResponse);
  
            currentNode.data.output = messageResponse;
            storedResponses[nodeId] = messageResponse;
            updateOutputNodes(structuredFlow, nodeId, messageResponse);
  
            // ğŸŒŸ Store response in MongoDB using agentFlowCRUD.js ğŸŒŸ
            await axios.post('https://j7-magic-tool.vercel.app/api/agentFlowCRUD', {
              flowId: executionId, // ğŸŒŸ Unique execution ID
              nodeId, // ğŸŒŸ Output node ID
              content: messageResponse,
              timestamp: new Date().toISOString() // ğŸŒŸ Add timestamp for cleanup
            });
            console.log(`ğŸ“¤ Stored response for Execution ID: ${executionId}, Output ID: ${nodeId}`);
  
          } catch (error) {
            console.error(`âŒ LLM Call Node (${nodeId}) Error:`, error);
            markNodeAsError(nodeId, error.message);
          }
        }
      }
    }
}

module.exports = executeLLMFlow;
