const fs = require('fs');
const path = require('path');
const axios = require('axios');

const channelId = process.env.SLACK_CHANNEL_ID || "x"; // Capture dynamically

async function executeLLMFlow(flowData) {
  console.log("üöÄ Starting Flow Execution...");

  if (!flowData || !flowData.length) {
    console.error("‚ùå No valid flowData received.");
    return;
  }

  // ‚úÖ Extract proper flow data structure
  flowData = flowData[0].flowData.drawflow;
  const executionOrder = determineExecutionOrder(flowData);
  const storedResponses = {}; // ‚úÖ Cache responses to avoid redundant API calls

  for (const nodeId of executionOrder) {
    const currentNode = flowData.Home.data[nodeId];

    if (!currentNode) continue;

    if (currentNode.name === 'LLM Call') {
      console.log(`üöÄ Processing LLM Call Node: ${nodeId}`);

      if (storedResponses[nodeId]) {
        console.log(`‚úÖ Using cached response for Node ${nodeId}`);
        currentNode.data.output = storedResponses[nodeId];
      } else {
        await waitForInputs(nodeId, flowData);

        const combinedInputs = getSortedInputs(nodeId, flowData);
        console.log("üìù Combined Inputs:", combinedInputs);

        const selectedModel = currentNode.data.selectedModel || 'openai/gpt-4o-mini';
        console.log(`üìù Model selected for Node ${nodeId}: ${selectedModel}`);

        try {
          const response = await callLLMAPI(combinedInputs, selectedModel);
          const messageResponse = response.data?.completions?.[selectedModel]?.completion?.choices?.[0]?.message?.content || "‚ö†Ô∏è No valid response";

          console.log(`‚úÖ LLM Call Node (${nodeId}) Response:`, messageResponse);

          currentNode.data.output = messageResponse;
          storedResponses[nodeId] = messageResponse;
          updateOutputNodes(flowData, nodeId, messageResponse);
        } catch (error) {
          console.error(`‚ùå LLM Call Node (${nodeId}) Error:`, error);
          markNodeAsError(nodeId, error.message);
        }
      }
    } else if (currentNode.name === 'Output') {
      console.log(`üì§ Processing Output Node: ${nodeId}`);

      const inputConnections = currentNode.inputs.input_1.connections.map(conn => conn.node);
      const linkedLLMNode = inputConnections.find(id => storedResponses[id]);

      if (linkedLLMNode) {
        const formattedResponse = formatTextAsHTML(storedResponses[linkedLLMNode]);
        currentNode.data.output = formattedResponse;

        console.log(`‚úÖ Output Node (${nodeId}) Displaying:`, formattedResponse);
        updateOutputNodes(flowData, nodeId, formattedResponse);
      } else {
        console.warn(`‚ö†Ô∏è Output Node (${nodeId}) has no valid LLM input.`);
      }
    }
  }

  // ‚úÖ Compile final output from all terminal nodes
  const finalOutputText = compileFinalOutputs(flowData);

  if (finalOutputText) {
    const filePath = generateOutputFile(finalOutputText);
    await sendSlackMessage(channelId, "‚úÖ Here's the final output:", filePath);
  }
}

// ‚úÖ Wait for valid input before proceeding
async function waitForInputs(nodeId, flowData) {
  return new Promise(resolve => {
    const checkInputs = () => {
      if (areInputsReady(nodeId, flowData)) {
        console.log(`‚úÖ Inputs for Node ${nodeId} are now ready!`);
        resolve();
      } else {
        console.log(`‚è≥ Waiting for inputs for Node ${nodeId}...`);
        setTimeout(checkInputs, 2000);
      }
    };
    checkInputs();
  });
}

// ‚úÖ Ensure inputs are ready before processing
function areInputsReady(nodeId, flowData) {
  const node = flowData.Home.data[nodeId];
  const inputConnections = Object.values(node.inputs).flatMap(input => input.connections).map(conn => conn.node);

  for (const inputNodeId of inputConnections) {
    const inputNode = flowData.Home.data[inputNodeId];
    let outputData = inputNode?.data?.output?.trim() || "";

    if (!outputData || outputData === "Waiting for response...") {
      console.log(`‚ùå Node ${nodeId} is waiting for input from Node ${inputNodeId}`);
      return false;
    }
  }
  return true;
}

// ‚úÖ Send final output to Slack
async function sendSlackMessage(channelId, message, filePath) {
  console.log(`üì© Sending message to Slack (${channelId})`);
  const slackToken = process.env.SLACK_BOT_TOKEN;

  await axios.post('https://slack.com/api/chat.postMessage', {
    channel: channelId,
    text: message
  }, {
    headers: { 'Authorization': `Bearer ${slackToken}`, 'Content-Type': 'application/json' }
  });

  console.log("‚úÖ Message sent to Slack.");
}

// ‚úÖ Call LLM API
async function callLLMAPI(prompt, model) {
  try {
    const response = await axios.post('https://j7-magic-tool.vercel.app/api/agentFlowStraicoCall', {
      message: prompt,
      models: model
    });
    return response.data;
  } catch (error) {
    throw new Error(`Error calling LLM API: ${error.message}`);
  }
}

// ‚úÖ Export function
module.exports = executeLLMFlow;
